# Week 4 Homework
--
1. Answer the following questions from the paper you read last week.

(a) Give an example of a set of *n*-grams for a word that are used in this work.    

(b)  Describe the feature space and label space for this problem.

(c) Describe at least two things about the paper that you did not understand.

(d) Are you surprised by the results of this paper?  Why or why not?

(e) What is the IG function and its relationship to entropy?

(f)  What is the prior in the IG function and how is it estimated?

(g)  The arg max function in the Na√Øve Bayes formula returns C.  What is it?  What would the max function return, if it were used instead of arg max?

(h) What reason do the authors give for the inconclusive results of boosted SVMs?

(i)  In statistics, features are called "dependent variables." In data mining, features are often called "attributes." 

(j) What criterion is typically used to create splitting nodes in the C4.5 algorithm mentioned in this paper?  (2) What is its relationship to KL-divergence?  And (3), what is this metric's relationship to entropy?

2. What is the loss function for a Naive Bayes classifier?  Explain why.

3. Does logistic regression have the same problem with zeroes as (non-smoothed) Naive Bayes?  Explain why.
